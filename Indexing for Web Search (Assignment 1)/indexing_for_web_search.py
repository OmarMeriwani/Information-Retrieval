# -*- coding: utf-8 -*-
"""Indexing for Web Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BqCshZAU1mBJ8NNZ1I1aZ4yQvtBb1SHN
"""

import numpy as np
import pandas as pd
import re
import sys
from bs4 import BeautifulSoup, NavigableString, Tag
from urllib.request import urlopen
import nltk
import requests

relations = []
#'Page','Location','Word','Vector_representation','Relative_Position','IsNER','InLink','Relations','RelatedWords','General_Rank','Relative_Rank'

'''Defining URLS'''
url1 = "http://csee.essex.ac.uk/staff/udo/index.html"
url2 = "https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering"

'''Reading URLs' content'''
Page1 = urlopen(url1.strip('\'"'))
Page2 = urlopen(url2.strip())
Page1 = BeautifulSoup(Page1)
Page2 = BeautifulSoup(Page2)

'''Content'''
links = []
headers = []
sentences = []
tokens = []

'''===================
Get links
===================='''
for link in Page1.findAll('a', attrs={'href': re.compile("^http://")}):
    text = re.sub('\s+',' ',link.text)
    linkObject = [link.get('href'), text]
    links.append(linkObject)
    #print(linkObject)
for link in Page1.findAll('a', attrs={'href': re.compile("^https://")}):
    text = re.sub('\s+',' ',link.text)
    linkObject = [link.get('href'), text]
    links.append(linkObject)
print(links)

'''===================
Get Headers
===================='''

hdrs = Page1.find_all(lambda tag: tag and tag.name.startswith("h"))
for header in hdrs:
    headers.append(header)
'''===================='''

from nltk.tokenize import sent_tokenize
nltk.download('punkt')

'''Pure text processing after getting links'''
Page1Text = re.sub('\s+',' ',Page1.text)
#print(Page1Text)
'''===================
Sentences tokenization
===================='''

sntnss = sent_tokenize(Page1Text.strip())
depth = 0
for s in sntnss:
  sentences.append([s,depth])
  depth = depth + 1
for s in sentences:
  print(s)

from nltk.tokenize import word_tokenize

'''===================
Words tokenization
===================='''
words = word_tokenize(Page1Text)
depth = 0
for w in words:
  tokens.append([w,depth])
  depth = depth + 1
print (tokens)

nltk.download('averaged_perceptron_tagger')
from nltk.corpus import stopwords
nltk.download('stopwords')

'''===================
POS tagging & Remove stop-words
===================='''
word_list=[]
for t in tokens:
  if t[0].isalpha():
    word_list.append(t[0])
#print(word_list)
word_list = [word for word in word_list if word not in stopwords.words('english')]  
print(word_list)
tagged_tokens = nltk.pos_tag(word_list)
print(tagged_tokens)

Nouns = []
for tag in tagged_tokens:
  if ('NN' in tag[1]):
    Nouns.append(tag[0])
print(Nouns)

'''===================
Multi-words expressions
===================='''
MWE = []
print(tagged_tokens)
def extract_phrases(my_tree, phrase):
  my_phrases = []
  if my_tree.label() == phrase:
    my_phrases.append(my_tree.copy(True))
  for child in my_tree:
    if type(child) is nltk.Tree:
      list_of_phrases = extract_phrases(child, phrase)
      if len(list_of_phrases) > 0:
        my_phrases.extend(list_of_phrases)
  return my_phrases
grammar = "NP: {<DT>?<JJ>*<NN>|<NNP>*}"
cp = nltk.RegexpParser(grammar)
tree = cp.parse(tagged_tokens)
list_of_noun_phrases = extract_phrases(tree, 'NP')
for phrase in list_of_noun_phrases:
  MWE.append(" ".join([x[0] for x in phrase.leaves()]))

Small_MWE = []
for i in MWE:
  if i.count(' ') < 3 and i.count(' ') > 1:
    Small_MWE.append(i)
MWE = Small_MWE
  
print(MWE)

'''===================
NER
===================='''
import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()
NER = nlp(Page1Text)
print([(X.text, X.label_) for X in NER.ents if (X.label_ == 'ORG' or X.label_ == 'PERSON'  or X.label_ == 'GPE')])
#Remove money
print(NER.ents)
NER_Words = [(X.text) for X in NER.ents]
NER_Less_than_4 = []
for i in NER_Words:
  if i.count(' ') < 3:
    NER_Less_than_4.append(i)
NER_Words = NER_Less_than_4
print(NER_Words)
df = pd.DataFrame(NER_Words)
df.to_csv('NER.csv')

"""Now for each MWE, NER, Noun, we will check the following:


*   If it exists in links which raises its important more.
*   If it exists in headers.
* If it exists in on the top of the page.
* The frequency of it.

Finally, a sorted list will be produced, that contains the most important nouns, NER and MWE.
"""

'''===================
Relations
====================
Relations = []
for sentence in sentences:
  grammar = r"""
  NP: {<DT|JJ|NN|PRP.*>+}      
  PP: {<IN><NP>}               
  VP: {<VB.*><NP|PP|CLAUSE>+$} 
  CLAUSE: {<NP><VP>}           
  """
  ww = word_tokenize(sentence[0])
  taggedww = nltk.pos_tag(ww)
  cp = nltk.RegexpParser(grammar)
  parsed_sentence = cp.parse(taggedww)
  print(parsed_sentence)'''

'''===================
Normalization
===================='''
from sklearn.feature_extraction.text import TfidfVectorizer
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.stem.porter import PorterStemmer

tf = TfidfVectorizer(stop_words='english')
tfs = tf.fit_transform(word_list)
result = tf.transform(['Udo Kruschwitz'])
print(result)
feature_names = tf.get_feature_names()
for col in result.nonzero()[1]:
    print (result[0, col])
FullList = []
MWE = np.array(MWE)
NER_Words = np.array(NER_Words)
Nouns = np.array(Nouns)
Full_List = np.unique(np.concatenate((MWE,NER_Words),0))

Full_List = np.unique(np.concatenate((Full_List,Nouns),0))
#print(Full_List)

'''===================
Index building
===================='''

from google.colab import files

Weights = []
for keyword in Full_List.tolist():
  max = 0
  result = tf.transform([keyword])
  feature_names = tf.get_feature_names()
  for col in result.nonzero()[1]:
    if (result[0, col]) > max:
      max = result[0, col] 
  links_Score = 0
  for l, text in links:
    if keyword in text:
      links_Score += 1
  headers_score = 0
  for h in headers:
    if keyword in h:
      headers_score += 1
  MinDepth = 1000000
  for s in sentences:
    #print(s[1])
    if keyword in s[0]:
      MinDepth = s[1]
  Total_Count = Page1Text.count(keyword) 
  Weights.append([keyword,max,links_Score,headers_score,MinDepth,Total_Count])
df = pd.DataFrame(Weights)
df.to_csv('Keywords_Weights.csv')
print (Weights)